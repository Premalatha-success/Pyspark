{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc302f82-d991-4601-a917-fd1f84578f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/nivimachinelearning@gmail.com/titanic_training_data-3.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a97e3d1-0154-4f1b-8384-b8c25a7bd4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+----+---+-----+-----+---------+----+-----+--------+\n|PassengerId|Survived|Pclass|                Name| Sex|Age|SibSp|Parch|   Ticket|Fare|Cabin|Embarked|\n+-----------+--------+------+--------------------+----+---+-----+-----+---------+----+-----+--------+\n|          1|       0|     3|Braund, Mr. Owen ...|male| 22|    1|    0|A/5 21171|7.25| null|       S|\n+-----------+--------+------+--------------------+----+---+-----+-----+---------+----+-----+--------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d608cd4-bb5a-4b58-8ca6-b21a1ebff7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- PassengerId: string (nullable = true)\n |-- Survived: string (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: string (nullable = true)\n |-- SibSp: string (nullable = true)\n |-- Parch: string (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: string (nullable = true)\n |-- Cabin: string (nullable = true)\n |-- Embarked: string (nullable = true)\n\nNumber of rows: 891\n+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|                0|  A10|       C|\n|    max|               99|                  1|                 3|van Melkebeke, Mr...|  male|                 9|                 8|                  6|         WE/P 5735|             93.5|    T|       S|\n+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n\n+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+\n|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|    Ticket|   Fare|Cabin|Embarked|\n+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+\n|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|  PC 17599|71.2833|  C85|       C|\n|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|    113803|   53.1| C123|       S|\n|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|    373450|   8.05| null|       S|\n|          7|       0|     1|McCarthy, Mr. Tim...|  male| 54|    0|    0|     17463|51.8625|  E46|       S|\n|         12|       1|     1|Bonnell, Miss. El...|female| 58|    0|    0|    113783|  26.55| C103|       S|\n|         14|       0|     3|Andersson, Mr. An...|  male| 39|    1|    5|    347082| 31.275| null|       S|\n|         16|       1|     2|Hewlett, Mrs. (Ma...|female| 55|    0|    0|    248706|     16| null|       S|\n|         19|       0|     3|Vander Planke, Mr...|female| 31|    1|    0|    345763|     18| null|       S|\n|         21|       0|     2|Fynney, Mr. Joseph J|  male| 35|    0|    0|    239865|     26| null|       S|\n|         22|       1|     2|Beesley, Mr. Lawr...|  male| 34|    0|    0|    248698|     13|  D56|       S|\n|         26|       1|     3|Asplund, Mrs. Car...|female| 38|    1|    5|    347077|31.3875| null|       S|\n|         31|       0|     1|Uruchurtu, Don. M...|  male| 40|    0|    0|  PC 17601|27.7208| null|       C|\n|         34|       0|     2|Wheadon, Mr. Edwa...|  male| 66|    0|    0|C.A. 24579|   10.5| null|       S|\n|         36|       0|     1|Holverson, Mr. Al...|  male| 42|    1|    0|    113789|     52| null|       S|\n|         41|       0|     3|Ahlin, Mrs. Johan...|female| 40|    1|    0|      7546|  9.475| null|       S|\n|         53|       1|     1|Harper, Mrs. Henr...|female| 49|    1|    0|  PC 17572|76.7292|  D33|       C|\n|         55|       0|     1|Ostby, Mr. Engelh...|  male| 65|    0|    1|    113509|61.9792|  B30|       C|\n|         62|       1|     1| Icard, Miss. Amelie|female| 38|    0|    0|    113572|     80|  B28|    null|\n|         63|       0|     1|Harris, Mr. Henry...|  male| 45|    1|    0|     36973| 83.475|  C83|       S|\n|         71|       0|     2|Jenkin, Mr. Steph...|  male| 32|    0|    0|C.A. 33111|   10.5| null|       S|\n+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# Display DataFrame schema\n",
    "df.printSchema()\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "rowCount = df.count()\n",
    "print(f\"Number of rows: {rowCount}\")\n",
    "\n",
    "# Display summary statistics for numeric columns\n",
    "df.describe().show()\n",
    "\n",
    "# Filter and display passengers with age greater than 30\n",
    "filtered_df = df.filter(df[\"Age\"] > 30)\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b5a2a8-70c3-4145-86cb-8db84d6eb4f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n38\n26\n35\n35\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# Extract a specific column as an RDD\n",
    "age_rdd = df.select(\"Age\").rdd.map(lambda row: row[0])\n",
    "\n",
    "# Display the first few elements of the RDD\n",
    "for age in age_rdd.take(5):\n",
    "    print(age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19fe1ac4-3c1e-45e8-8c42-c3f8bec1e747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Age: 23.79929292929293\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# Convert age values to numbers in the RDD, handling None values and non-numeric values\n",
    "age_number_rdd = age_rdd.map(lambda age: float(age) if age is not None and age.replace('.', '', 1).isdigit() else 0)\n",
    "\n",
    "# Calculate the average age using RDD\n",
    "total_age = age_number_rdd.reduce(lambda x, y: x + y)\n",
    "average_age = total_age / rowCount\n",
    "print(f\"Average Age: {average_age}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cddae0a-24f3-4996-aa2b-2c4863f6b59b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|Pclass|count|\n+------+-----+\n|     1|  216|\n|     2|  184|\n|     3|  491|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# Register DataFrame as a temporary SQL table\n",
    "df.createOrReplaceTempView(\"titanic_table\")\n",
    "\n",
    "# Perform SQL query on DataFrame\n",
    "sql_result = spark.sql(\"SELECT Pclass, COUNT(*) AS count FROM titanic_table GROUP BY Pclass ORDER BY Pclass\")\n",
    "sql_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4ec774-1d39-452d-a620-1b80bf29dfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">filePath: String = /FileStore/shared_uploads/nivimachinelearning@gmail.com/titanic_training_data-3.csv\n",
       "df: org.apache.spark.sql.DataFrame = [PassengerId: string, Survived: string ... 10 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">filePath: String = /FileStore/shared_uploads/nivimachinelearning@gmail.com/titanic_training_data-3.csv\ndf: org.apache.spark.sql.DataFrame = [PassengerId: string, Survived: string ... 10 more fields]\n</div>",
       "datasetInfos": [
        {
         "name": "df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "PassengerId",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Survived",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Pclass",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Sex",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Age",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "SibSp",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Parch",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Ticket",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Fare",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Cabin",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "Embarked",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Define the file path\n",
    "val filePath = \"/FileStore/shared_uploads/nivimachinelearning@gmail.com/titanic_training_data-3.csv\"\n",
    "\n",
    "// Read CSV into a DataFrame\n",
    "val df = spark.read.option(\"header\", \"true\").csv(filePath)\n",
    "\n",
    "// Create a temporary SQL table\n",
    "df.createOrReplaceTempView(\"titanic_table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "376fb246-88eb-46b0-a6fc-814b8e009585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------+-----+\n",
       "Pclass|count|\n",
       "+------+-----+\n",
       "     1|  216|\n",
       "     2|  184|\n",
       "     3|  491|\n",
       "+------+-----+\n",
       "\n",
       "sqlResult: org.apache.spark.sql.DataFrame = [Pclass: string, count: bigint]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------+-----+\n|Pclass|count|\n+------+-----+\n|     1|  216|\n|     2|  184|\n|     3|  491|\n+------+-----+\n\nsqlResult: org.apache.spark.sql.DataFrame = [Pclass: string, count: bigint]\n</div>",
       "datasetInfos": [
        {
         "name": "sqlResult",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "Pclass",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "count",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Perform SQL query on DataFrame\n",
    "val sqlResult = spark.sql(\"SELECT Pclass, COUNT(*) AS count FROM titanic_table GROUP BY Pclass ORDER BY Pclass\")\n",
    "sqlResult.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98662be6-7974-461d-af50-5e920a558d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">1\n",
       "4\n",
       "9\n",
       "16\n",
       "25\n",
       "data: List[Int] = List(1, 2, 3, 4, 5)\n",
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[180] at parallelize at command-3727048001105416:3\n",
       "squaredRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[181] at map at command-3727048001105416:6\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">1\n4\n9\n16\n25\ndata: List[Int] = List(1, 2, 3, 4, 5)\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[180] at parallelize at command-3727048001105416:3\nsquaredRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[181] at map at command-3727048001105416:6\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Create an RDD from a collection\n",
    "val data = List(1, 2, 3, 4, 5)\n",
    "val rdd = sc.parallelize(data)\n",
    "\n",
    "// Perform a transformation: Square each element\n",
    "val squaredRDD = rdd.map(x => x * x)\n",
    "\n",
    "// Perform an action: Print the squared values\n",
    "squaredRDD.collect().foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99de1a2c-8c50-4309-85a3-17d15e0487fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+-------+\n",
       " ID|   Name|\n",
       "+---+-------+\n",
       "  2|    Bob|\n",
       "  3|Charlie|\n",
       "+---+-------+\n",
       "\n",
       "data: List[(Int, String)] = List((1,Alice), (2,Bob), (3,Charlie))\n",
       "df: org.apache.spark.sql.DataFrame = [ID: int, Name: string]\n",
       "resultDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Name: string]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+---+-------+\n| ID|   Name|\n+---+-------+\n|  2|    Bob|\n|  3|Charlie|\n+---+-------+\n\ndata: List[(Int, String)] = List((1,Alice), (2,Bob), (3,Charlie))\ndf: org.apache.spark.sql.DataFrame = [ID: int, Name: string]\nresultDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Name: string]\n</div>",
       "datasetInfos": [
        {
         "name": "df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "ID",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "Name",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        },
        {
         "name": "resultDF",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "ID",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "Name",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Create a DataFrame from a collection\n",
    "val data = List((1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"))\n",
    "val df = spark.createDataFrame(data).toDF(\"ID\", \"Name\")\n",
    "\n",
    "// Perform transformations using DataFrame API\n",
    "val resultDF = df.select(\"ID\", \"Name\").filter(\"ID > 1\")\n",
    "\n",
    "// Show the result\n",
    "resultDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a07de5cd-4d5b-443c-8b82-b5545497248d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RDD and Dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
